# -*- coding: utf-8 -*-
"""LiverDieseaseDataPreprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SQqohjuP-Io6czcUdRhoGAU3_oFwwtMt
"""

from google.colab import files
data_to_load = files.upload()

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

data = pd.read_csv("indian_liver_patient.csv")
data.head()

data.info()

classCount = pd.value_counts(data['Dataset'], sort = True).sort_index()
classCount.plot(kind = 'bar')
plt.title("Liver disease classes")
plt.xlabel("Dataset")
plt.ylabel("Frequency")

data['Dataset'] = data['Dataset'].map({2:0,1:1})

data['Dataset'].value_counts()

classCount = pd.value_counts(data['Dataset'], sort = True).sort_index()
classCount.plot(kind = 'bar')
plt.title("Liver disease classes")
plt.xlabel("Dataset")
plt.ylabel("Frequency")

duplicates = data[data.duplicated(keep=False)]
duplicates

print("no. of duplicate values: ", duplicates.shape[0], "\nSize of data: ", data.shape)

data = data[~data.duplicated(subset=None, keep='first')]
duplicates = data[data.duplicated(keep=False)]
duplicates

print("No. of duplicate values: 0\nSize of data: ", data.shape)

data.isna().sum()

data['Albumin_and_Globulin_Ratio'].fillna(value=0, inplace=True)

data.isna().sum()

dataFeatures=data.drop(['Dataset'],axis=1)
dataNumFeatures=data.drop(['Gender','Dataset'],axis=1)
dataNumFeatures.head()

dataNumFeatures.describe()

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
cols=list(dataNumFeatures.columns)
dataFeaturesScaled=pd.DataFrame(data=dataFeatures)
dataFeaturesScaled[cols]=scaler.fit_transform(dataFeatures[cols])
dataFeaturesScaled.head()

dataExp=pd.get_dummies(dataFeaturesScaled)
dataExp.head()

f, ax = plt.subplots(figsize=(12, 10))
plt.title('Pearson Correlation of liver disease Features')
# Draw the heatmap using seaborn
sns.heatmap(dataNumFeatures.astype(float).corr(),linewidths=0.25,vmax=1.0, square=True, cmap="YlGnBu", linecolor='black',annot=True)

from sklearn.model_selection import train_test_split
X=dataExp
y=data['Dataset'] 
X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.3,random_state=0)

X.hist(figsize=(15, 10))
plt.ylabel("Frequency")
plt.show()

sns.catplot(x="Gender", col="Dataset", data = data, kind="count", height=5, aspect=.8)

sns.catplot(x="Age", col="Dataset", data = data, kind="count", height=20, aspect = 1.0)

"""**Random Forest Classifier**"""

from sklearn.ensemble import RandomForestClassifier
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import GridSearchCV
param_grid = {'n_estimators': range(100, 201, 5), 'criterion':['gini', 'entropy'], 'max_features':['auto', 'sqrt', 'log2', None]}

tuning = GridSearchCV(estimator=RandomForestClassifier(), param_grid= param_grid, cv = 5, verbose = 2, n_jobs=1, scoring = 'f1')
tuning.fit(X_train, Y_train)
tuning.best_params_, tuning.best_score_

classifier = RandomForestClassifier(n_estimators=80, criterion = 'gini', random_state=10, max_features='sqrt')

model = classifier.fit(X_train, Y_train)
Y_pred = model.predict(X_test)

from sklearn.model_selection import cross_val_score
score = cross_val_score(model, X_train, Y_train, cv=10)
print("Maximum Accuracy: ", round(max(score)*100, 2), "%")
print("Average Accuracy: ", round(score.mean()*100,2), "%")

from sklearn.metrics import plot_roc_curve
plot_roc_curve(model, X_test, Y_test)

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
print(confusion_matrix(Y_test, Y_pred))
print(classification_report(Y_test, Y_pred))

"""**Logistic Regression**"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_Train = sc.fit_transform(X_train)
X_Test = sc.transform(X_test)

from sklearn.linear_model import LogisticRegression
import warnings 
warnings.filterwarnings('ignore')

paramgrid = [{'max_iter': range(1,30,1), 'solver': ['liblinear', 'saga', 'lbfgs']}]

from sklearn.model_selection import GridSearchCV
tuningLR = GridSearchCV(estimator=LogisticRegression(), param_grid=paramgrid, cv=5, verbose=True, scoring='f1')
tuningLR.fit(X_train, Y_train)
tuningLR.best_params_,tuningLR.best_score_

modelLR = LogisticRegression(max_iter=10, solver='newton-cg', random_state=10)
modelLR.fit(X_Train, Y_train)
ypred = modelLR.predict(X_Test)

from sklearn.model_selection import cross_val_score
score = cross_val_score(modelLR, X_Train, Y_train, cv=10)
score

print("Maximum Accuracy: ", round(max(score)*100,2), "%")
print("Average Accuracy: ", round(score.mean()*100,2), "%")

from sklearn.metrics import plot_roc_curve
 plot_roc_curve(modelLR, X_Test, Y_test)

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
print(confusion_matrix(Y_test, Y_pred))
print(classification_report(Y_test, Y_pred))

